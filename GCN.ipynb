{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7eec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    ")\n",
    "from src.losses.focal_loss import FocalLoss, reweight\n",
    "from main import load_dataset, create_loader, test_model, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, adj=None):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    loader: NeighborLoader,\n",
    "    learning_rate=0.001,\n",
    "    device=\"cpu\",\n",
    "    num_epochs=5,\n",
    "    output_directory=\"./outputs\",\n",
    "    timestamp: str = None,\n",
    "    loss_type: str = \"focal\", # 'focal' or 'ce'\n",
    "    per_cls_weights: torch.Tensor = None,\n",
    "    gamma: float = 1.0,\n",
    ") -> Dict[str, list]:\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "\n",
    "    Args:\n",
    "        model: The model to train\n",
    "        loader: Data loader for training data\n",
    "        learning_rate (float): Learning rate for the optimizer\n",
    "        device: Device to train on\n",
    "        num_epochs (int): Number of epochs to train\n",
    "        save_dir (str): Directory to save model and logs\n",
    "\n",
    "    Returns:\n",
    "        dict: Training metrics including loss histories\n",
    "    \"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training histories\n",
    "    train_histories = {\n",
    "        'loss': []\n",
    "    }\n",
    "    \n",
    "    # Test histories (will be populated during evaluation if performed during training)\n",
    "    test_histories = {\n",
    "        'loss': []\n",
    "    }\n",
    "    \n",
    "    # Classification metrics histories\n",
    "    metrics_histories = {\n",
    "        'auc': [],\n",
    "        'accuracy': [],\n",
    "        'f1': [],\n",
    "        'precision': [],\n",
    "        'recall': []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        train_total_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            x = batch.x\n",
    "            y = batch.y\n",
    "            edge_index = batch.edge_index\n",
    "            batch_size = getattr(batch, \"batch_size\", x.size(0))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x, edge_index) # use full structure to allow message passing\n",
    "\n",
    "            # filter with train_mask to only compute loss with known labels \n",
    "            train_out, train_labels = out[batch.train_mask], batch.y[batch.train_mask] #[:, :2]\n",
    "\n",
    "            # check unique labels\n",
    "            # print(f\"unique labels in batch (torch.unique(batch.y)): {torch.unique(batch.y)}\")\n",
    "            # print(f\"unique labels filtered (torch.unique(batch.y[batch.train_mask])): {torch.unique(train_labels)}\")\n",
    "\n",
    "            per_cls_weights = reweight(torch.bincount(train_labels).tolist())\n",
    "\n",
    "            if loss_type == 'focal':\n",
    "                if len(per_cls_weights) < 2:\n",
    "                    print(f\"Warning: skiping batch with insufficient classes, {per_cls_weights}\")\n",
    "                    continue\n",
    "                print(f\"training with focal, weights {per_cls_weights}, gamma {gamma}\")\n",
    "                criterion = FocalLoss(weight=per_cls_weights, gamma=gamma)\n",
    "                loss = criterion(\n",
    "                    train_out, train_labels)\n",
    "\n",
    "            else:\n",
    "                print(\"training with ce\")\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "                loss = criterion(\n",
    "                    train_out, train_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), max_norm=1.0\n",
    "            )  # gradient clipping - eric\n",
    "            optimizer.step()\n",
    "            train_total_loss += loss.item()\n",
    "\n",
    "        batch_count = len(loader)  # Total number of batches\n",
    "        avg_loss = train_total_loss / batch_count\n",
    "        print(\n",
    "            f\"Avg Batch Loss: {avg_loss:.3e}, \"\n",
    "        )\n",
    "\n",
    "        # Record training epoch metrics\n",
    "        train_histories['loss'].append(avg_loss)\n",
    "\n",
    "    # timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M\")\n",
    "    model_path = os.path.join(output_directory, f\"dominant_model_{timestamp}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"loss_history\": train_histories['loss'],\n",
    "        # \"attr_loss_history\": train_histories['attr_loss'],\n",
    "        # \"struct_loss_history\": train_histories['struct_loss'],\n",
    "        # \"alpha_history\": train_histories['alpha'],\n",
    "    }\n",
    "\n",
    "    metrics_path = os.path.join(output_directory, f\"train_metrics_{timestamp}.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            {k: [float(v) for v in vals] for k, vals in metrics.items()}, f\n",
    "        )  # apparently json doesn't like torch tensors\n",
    "    print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(\n",
    "    loss_history,\n",
    "    # attr_loss_history,\n",
    "    # struct_loss_history,\n",
    "    output_directory=\"./outputs\",\n",
    "    timestamp: str = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the training loss\n",
    "\n",
    "    Args:\n",
    "        loss_history: The loss history\n",
    "        attr_loss_history: The attribute loss history\n",
    "        struct_loss_history: The structure loss history\n",
    "        output_directory (str): Directory to save the plot\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history, \"o-\", label=\"Total Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(\"Training Loss History\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, f\"loss_plot_{timestamp}.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73831e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: GCN,\n",
    "    data,\n",
    "    device,\n",
    "    batch_size=2048,\n",
    "    num_neighbors=[10, 10],\n",
    "    output_directory=\"./outputs\",\n",
    "    threshold=0.5,\n",
    "    timestamp: str = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the DOMINANT model on the test dataset\n",
    "\n",
    "    Args:\n",
    "        model: The trained model\n",
    "        data: The dataset\n",
    "        device: Device to evaluate on\n",
    "        batch_size: Batch size for evaluation\n",
    "        num_neighbors: Number of neighbors to sample\n",
    "        output_directory: Directory to save results\n",
    "        threshold: Threshold for anomaly detection\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Placeholder for evaluation logic\n",
    "    # You may want to implement a proper evaluation function\n",
    "    # For now, we will just return dummy metrics\n",
    "    return {\n",
    "        \"auc\": 0.0,\n",
    "        \"accuracy\": 0.0,\n",
    "        \"f1\": 0.0,\n",
    "        \"precision\": 0.0,\n",
    "        \"recall\": 0.0,\n",
    "    }\n",
    "\n",
    "def test_model(\n",
    "    model: GCN,\n",
    "    data,\n",
    "    device,\n",
    "    batch_size=2048,\n",
    "    num_neighbors=[10, 10],\n",
    "    output_directory=\"./outputs\",\n",
    "    threshold=0.5,\n",
    "    timestamp: str = None,\n",
    "    loss_type: str = \"focal\",  # 'focal' or 'ce'\n",
    "    gamma: float = 1.0,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Test the DOMINANT model on the test dataset\n",
    "\n",
    "    Args:\n",
    "        model: The trained model\n",
    "        data: The dataset\n",
    "        device: Device to test on\n",
    "        batch_size: Batch size for testing\n",
    "        num_neighbors: Number of neighbors to sample\n",
    "        output_directory: Directory to save results\n",
    "        threshold: Threshold for anomaly detection\n",
    "\n",
    "    Returns:\n",
    "        dict: Test metrics\n",
    "    \"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # print(f\"Model input dimension: {model.shared_encoder.convs[0].in_channels}\")\n",
    "    print(f\"Data feature dimension: {data.num_node_features}\")\n",
    "    # print(f\"Using aggregation: {model.use_aggregation}\")\n",
    "    # if model.use_aggregation:\n",
    "    #     print(\n",
    "    #         f\"Aggregation methods - Mean: {model.aggregation_mean}, Max: {model.aggregation_max}\"\n",
    "    #     )\n",
    "    #     expected_dim = data.num_node_features\n",
    "    #     if model.aggregation_mean:\n",
    "    #         expected_dim += data.num_node_features\n",
    "    #     if model.aggregation_max:\n",
    "    #         expected_dim += data.num_node_features\n",
    "    #     print(f\"Expected input dimension after aggregation: {expected_dim}\")\n",
    "\n",
    "    # Create loader for test data\n",
    "    loader = create_loader(\n",
    "        data, batch_size=batch_size, num_neighbors=num_neighbors, use_train_mask=False\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_losses = []\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Testing\"):\n",
    "            batch = batch.to(device)\n",
    "            x = batch.x\n",
    "            y = batch.y\n",
    "            edge_index = batch.edge_index\n",
    "            batch_size = getattr(batch, \"batch_size\", x.size(0))\n",
    "\n",
    "            # Forward pass through the model\n",
    "            out = model(x, edge_index)\n",
    "\n",
    "            # Get predictions\n",
    "            probs = torch.softmax(out, dim=1)\n",
    "            scores = probs[:, 1]  # Assuming binary classification\n",
    "\n",
    "            test_mask = getattr(batch, \"test_mask\", torch.ones_like(y, dtype=torch.bool))\n",
    "            if test_mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            y_true = y[test_mask]\n",
    "            # print(f\"Unique labels in test_mask: {torch.unique(y_true)}\")\n",
    "            y_pred_logits = out[test_mask]#[:, :2]\n",
    "            y_scores = scores[test_mask]\n",
    "\n",
    "            # test_out, test_labels = out[batch.test_mask][:, :2], batch.y[batch.test_mask]\n",
    "\n",
    "            # Handle class imbalance (optional in test)\n",
    "            if loss_type == 'focal':\n",
    "                per_cls_weights = reweight(torch.bincount(y_true).tolist())\n",
    "                if len(per_cls_weights) < 2:\n",
    "                    print(f\"Warning: skiping batch with insufficient classes: {per_cls_weights}\")\n",
    "                    continue\n",
    "                print(f\"testing with focal, weights {per_cls_weights}, gamma {gamma}\")\n",
    "                criterion = FocalLoss(weight=per_cls_weights.to(device), gamma=gamma)\n",
    "            else:\n",
    "                print(\"testing with ce\")\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            loss = criterion(y_pred_logits, y_true)\n",
    "            all_losses.append(loss.item())\n",
    "            all_scores.append(y_scores.cpu().numpy())\n",
    "            all_labels.append(y_true.cpu().numpy())\n",
    "\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Filter out unknown labels (class 2)\n",
    "    mask = all_labels != 2\n",
    "    scores = all_scores[mask]\n",
    "    labels = all_labels[mask]\n",
    "\n",
    "    \"\"\"The nodes are then ranked according to their anomaly scores in\n",
    "    descending order, and the top-k nodes are recognized as anoma-\n",
    "    lies - page 7 in https://arxiv.org/pdf/2106.07178\"\"\"\n",
    "\n",
    "    sort_indices = np.argsort(scores)[::-1] # descending order\n",
    "    sorted_scores = scores[sort_indices]\n",
    "    #print(sorted_scores[:50])\n",
    "    sorted_labels = labels[sort_indices]\n",
    "    #print(sorted_labels[:50])\n",
    "\n",
    "    threshold = np.mean(sorted_labels == 1) \n",
    "    print(f\"calculated threshold: {threshold:.3f}\")\n",
    "\n",
    "    num_samples = len(sorted_scores)\n",
    "    num_class_1 = int(num_samples * threshold)\n",
    "    predictions = np.zeros_like(sorted_labels)\n",
    "    predictions[0:num_class_1] = 1  # Mark top-k as anomalies\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(sorted_labels, predictions)\n",
    "    f1 = f1_score(sorted_labels, predictions)\n",
    "    precision = precision_score(sorted_labels, predictions)\n",
    "    recall = recall_score(sorted_labels, predictions)\n",
    "    classification_report_output = classification_report(sorted_labels, predictions)\n",
    "    classification_report_str = classification_report(\n",
    "        sorted_labels, predictions, output_dict=True\n",
    "    )\n",
    "    avg_loss = np.mean(all_losses)\n",
    "\n",
    "    metrics = {\n",
    "        #\"auc\": float(auc),\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"f1\": float(f1),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"loss\": float(avg_loss),\n",
    "        \"classification_report\": classification_report_str,\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    #print(f\"AUC: {metrics['auc']:.3f}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"F1: {metrics['f1']:.3f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"Loss: {metrics['loss']:.3e}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report_output)\n",
    "\n",
    "    # Save metrics\n",
    "    # timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M\")\n",
    "    metrics_path = os.path.join(output_directory, f\"test_metrics_{timestamp}.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "    print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config=None):\n",
    "    \"\"\"\n",
    "    Main function to run the training and testing pipeline\n",
    "\n",
    "    Args:\n",
    "        config (dict, optional): Configuration parameters\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {\n",
    "            # Model parameters\n",
    "            \"hidden_dim\": 64,\n",
    "            \"dropout\": 0.1,\n",
    "            # Training parameters\n",
    "            \"batch_size\": 2048,\n",
    "            \"num_neighbors\": [10, 10],\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"num_epochs\": 2,\n",
    "            # Paths for output\n",
    "            \"data_root\": \"data/elliptic\",\n",
    "            \"save_dir\": \"./saved_models\",\n",
    "            # threshold for our testing\n",
    "            \"threshold\": 0.9,\n",
    "            # transfer learning options\n",
    "            \"transfer_learning\": False,\n",
    "            \"load_model_path\": None,\n",
    "            # focal loss\n",
    "            \"loss_type\": \"focal\",\n",
    "            \"gamma\": 1.0\n",
    "        }\n",
    "\n",
    "    data = load_dataset(root=config[\"data_root\"])\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M\")\n",
    "    print(f\"Timestamp: {timestamp}\")\n",
    "\n",
    "\n",
    "    if config[\"load_model_path\"] is None:\n",
    "\n",
    "        model = GCN(data.num_features, config[\"hidden_dim\"], len(torch.unique(data.y)))\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "\n",
    "        train_loader = create_loader(\n",
    "            data,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            num_neighbors=config[\"num_neighbors\"],\n",
    "            use_train_mask=True,\n",
    "        )\n",
    "\n",
    "        print(\"Train loader input nodes:\")\n",
    "        print(train_loader.input_nodes.unique())\n",
    "\n",
    "\n",
    "        model, training_metrics = train_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            device=device,\n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            output_directory=config[\"save_dir\"],\n",
    "            timestamp=timestamp,\n",
    "            loss_type=config[\"loss_type\"],\n",
    "            gamma=config[\"gamma\"]\n",
    "        )\n",
    "        plot_loss(\n",
    "            training_metrics[\"loss_history\"],\n",
    "            output_directory=config[\"save_dir\"],\n",
    "            timestamp=timestamp,\n",
    "        )\n",
    "\n",
    "        test_metrics = test_model(\n",
    "            model,\n",
    "            data,\n",
    "            device,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            num_neighbors=config[\"num_neighbors\"],\n",
    "            output_directory=config[\"save_dir\"],\n",
    "            threshold=config[\"threshold\"],\n",
    "            timestamp=timestamp,\n",
    "            loss_type=config[\"loss_type\"],\n",
    "            gamma=config[\"gamma\"]\n",
    "        )\n",
    "        \n",
    "    # elif config[\"load_model_path\"] is not None:\n",
    "    #     print(f\"Loading pre-trained model from {config['load_model_path']}\")\n",
    "    #     model, device = load_model_for_transfer_learning(\n",
    "    #         model_path=config[\"load_model_path\"],\n",
    "    #         data=data,\n",
    "    #         config=config,)\n",
    "\n",
    "\n",
    "\n",
    "    # if config[\"transfer_learning\"]:\n",
    "    #     # Transfer learning logic here\n",
    "    #     train_test_transfer_learning(\n",
    "    #         model,\n",
    "    #         data,\n",
    "    #         device,\n",
    "    #         config=config,\n",
    "    #         timestamp=timestamp,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a7196",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"load_model_path\": None,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_neighbors\": [10, 10],\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_epochs\": 10,\n",
    "    \"data_root\": \"../data/elliptic\",\n",
    "    \"save_dir\": \"./saved_models\",\n",
    "    \"threshold\": 0.9,\n",
    "    \"loss_type\": \"focal\", # focal or ce\n",
    "    \"gamma\": 1.0\n",
    "}\n",
    "\n",
    "main(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-graph2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
