
from pygod.detector import DOMINANT as PyGDOMINANT
#from pygod.detector import DeepDetector
#from pygod.nn import DOMINANTBase
#import torch.nn.functional as F
import time
from inspect import signature
from abc import ABC, abstractmethod

import torch
import numpy as np
#from scipy.stats import binom
#from scipy.special import erf

#from torch_geometric.nn import GIN
from torch_geometric import compile
from torch_geometric.loader import NeighborLoader
from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score

from pygod.utils import logger


class DOMINANT(PyGDOMINANT):
    """
    Deep Anomaly Detection on Attributed Networks

    Patched Dominant for better analysis of loss. 
    See :cite:`ding2019deep` for details.

    Parameters
    ----------
    hid_dim :  int, optional
        Hidden dimension of model. Default: ``64``.
    num_layers : int, optional
       Total number of layers in model. A half (floor) of the layers
       are for the encoder, the other half (ceil) of the layers are
       for decoders. Default: ``4``.
    dropout : float, optional
        Dropout rate. Default: ``0.``.
    weight_decay : float, optional
        Weight decay (L2 penalty). Default: ``0.``.
    act : callable activation function or None, optional
        Activation function if not None.
        Default: ``torch.nn.functional.relu``.
    sigmoid_s : bool, optional
        Whether to use sigmoid function to scale the reconstructed
        structure. Default: ``False``.
    backbone : torch.nn.Module, optional
        The backbone of the deep detector implemented in PyG.
        Default: ``torch_geometric.nn.GCN``.
    contamination : float, optional
        The amount of contamination of the dataset in (0., 0.5], i.e.,
        the proportion of outliers in the dataset. Used when fitting to
        define the threshold on the decision function. Default: ``0.1``.
    lr : float, optional
        Learning rate. Default: ``0.004``.
    epoch : int, optional
        Maximum number of training epoch. Default: ``100``.
    gpu : int
        GPU Index, -1 for using CPU. Default: ``-1``.
    batch_size : int, optional
        Minibatch size, 0 for full batch training. Default: ``0``.
    num_neigh : int, optional
        Number of neighbors in sampling, -1 for all neighbors.
        Default: ``-1``.
    weight : float, optional
        Weight between reconstruction of node feature and structure.
        Default: ``0.5``.
    verbose : int, optional
        Verbosity mode. Range in [0, 3]. Larger value for printing out
        more log information. Default: ``0``.
    save_emb : bool, optional
        Whether to save the embedding. Default: ``False``.
    compile_model : bool, optional
        Whether to compile the model with ``torch_geometric.compile``.
        Default: ``False``.
    **kwargs : optional
        Additional arguments for the backbone.

    Attributes
    ----------
    decision_score_ : torch.Tensor
        The outlier scores of the training data. Outliers tend to have
        higher scores. This value is available once the detector is
        fitted.
    threshold_ : float
        The threshold is based on ``contamination``. It is the
        :math:`N \\times` ``contamination`` most abnormal samples in
        ``decision_score_``. The threshold is calculated for generating
        binary outlier labels.
    label_ : torch.Tensor
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers. It is generated by applying
        ``threshold_`` on ``decision_score_``.
    emb : torch.Tensor or tuple of torch.Tensor or None
        The learned node hidden embeddings of shape
        :math:`N \\times` ``hid_dim``. Only available when ``save_emb``
        is ``True``. When the detector has not been fitted, ``emb`` is
        ``None``. When the detector has multiple embeddings,
        ``emb`` is a tuple of torch.Tensor.
    """
    def __init__(self,  *args,  **kwds):
        super().__init__(*args, **kwds)
        
        self.losses_ = {
            "train": {"total": [], "attr": [], "struct": []},
            "eval":  {"total": [], "attr": [], "struct": []}
        }
        # Track per-node anomaly scores for train/eval
        self.scores_ = {
            "train": None,
            "eval": None
        }
       
    def forward_model(self, data):
        data.to(self.device)
        
        batch_size = data.batch_size
        node_idx = data.n_id

        x = data.x.to(self.device)
        s = data.s.to(self.device)
        edge_index = data.edge_index.to(self.device)

        x_, s_ = self.model(x, edge_index)

        score, attr_score, struct_score = double_recon_loss(x[:batch_size],
                                     x_[:batch_size],
                                     s[:batch_size, node_idx],
                                     s_[:batch_size],
                                     self.weight)
        
        loss = torch.mean(score)
        attr_loss = torch.mean(attr_score)
        struct_loss = torch.mean(struct_score)
    
        return loss, score.detach().cpu(), attr_loss, struct_loss
    
    def fit(self, data, label =None, eval_data = None, eval_label = None):

        ### loss tracking patch ###
        


        self.process_graph(data)
        self.eval_data = eval_data
        if self.eval_data is not None:
            self.process_graph(self.eval_data)

        self.num_nodes, self.in_dim = data.x.shape
        if self.batch_size == 0:
            self.batch_size = data.x.shape[0]
        loader = NeighborLoader(data,
                                self.num_neigh,
                                batch_size=self.batch_size)

        self.model = self.init_model(**self.kwargs)
        if self.compile_model:
            self.model = compile(self.model)
        if not self.gan:
            optimizer = torch.optim.Adam(self.model.parameters(),
                                         lr=self.lr,
                                         weight_decay=self.weight_decay)
        else:
            self.opt_in = torch.optim.Adam(self.model.inner.parameters(),
                                           lr=self.lr,
                                           weight_decay=self.weight_decay)
            optimizer = torch.optim.Adam(self.model.outer.parameters(),
                                         lr=self.lr,
                                         weight_decay=self.weight_decay)

        self.model.train()
        self.decision_score_ = torch.zeros(data.x.shape[0])
        for epoch in range(self.epoch):
            start_time = time.time()
            epoch_loss = 0
            attr_loss = 0
            struct_loss = 0
            if self.gan:
                self.epoch_loss_in = 0
            for sampled_data in loader:
                batch_size = sampled_data.batch_size
                node_idx = sampled_data.n_id

                loss, score, attr_loss, struct_loss = self.forward_model(sampled_data)
                epoch_loss += loss.item() * batch_size
                attr_loss += attr_loss.item() * batch_size
                struct_loss += struct_loss.item() * batch_size
                self.decision_score_[node_idx[:batch_size]] = score
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            # Save per-epoch mean losses for training
            loss_value = epoch_loss / data.x.shape[0]
            attr_losses = attr_loss / data.x.shape[0]
            struct_losses = struct_loss / data.x.shape[0]
            self.losses_["train"]["total"].append(float(loss_value))
            self.losses_["train"]["attr"].append(float(attr_losses))
            self.losses_["train"]["struct"].append(float(struct_losses))

            if self.gan:
                loss_value = (self.epoch_loss_in / data.x.shape[0], loss_value)
            logger(epoch=epoch,
                   loss=loss_value,
                   score=self.decision_score_,
                   target=label,
                   time=time.time() - start_time,
                   verbose=self.verbose,
                   train=True)

            
        
            if self.eval_data is not None:
                self.evaluate(epoch=epoch)
        self._process_decision_score()
        self.scores_["train"] = self.decision_score_.numpy()
        return self


    def evaluate(self, epoch):
        self.model.eval()
        with torch.no_grad():
            data = self.eval_data

            x = data.x.to(self.device)
            s = data.s.to(self.device)
            edge_index = data.edge_index.to(self.device)

            x_, s_ = self.model(x, edge_index)

            score, attr_loss, struct_loss = double_recon_loss(
                x,
                x_,
                s,
                s_,
                self.weight
            )
            loss = torch.mean(score)
            self.losses_["eval"]["total"].append(float(loss.item()))
            self.losses_["eval"]["attr"].append(float(attr_loss.mean().item()))
            self.losses_["eval"]["struct"].append(float(struct_loss.mean().item()))
            self.scores_["eval"] = score.cpu().numpy()
            logger(epoch=epoch,
                   loss=float(loss.item()),
                    verbose=self.verbose,
                   train=False)
        return loss.item(), score.detach().cpu()



def double_recon_loss(x, x_hat, s, s_hat, weight=0.5):
    """
    Simplified double reconstruction loss.
    Returns a tuple of (combined score, feature error, structure error),
    where the combined score is the weighted sum of the other errors. 

    Parameters
    ----------
    x       : torch.Tensor  # ground‐truth features
    x_hat   : torch.Tensor  # reconstructed features
    s       : torch.Tensor  # ground‐truth structure
    s_hat   : torch.Tensor  # reconstructed structure
    weight  : float         # balance between feature and structure losses

    Returns
    -------
    score       : torch.Tensor  # weight * feature_error + (1-weight) * structure_error
    attr_err : torch.Tensor  # sqrt of summed squared feature differences
    struct_err  : torch.Tensor  # sqrt of summed squared structure differences
    """
    # feature reconstruction error
    diff_attr    = (x - x_hat).pow(2)
    attr_err  = torch.sqrt(diff_attr.sum(dim=1))

    # structure reconstruction error
    diff_stru    = (s - s_hat).pow(2)
    struct_err   = torch.sqrt(diff_stru.sum(dim=1))

    # combined outlier score per node
    score = weight * attr_err + (1 - weight) * struct_err

    return score, attr_err, struct_err


if __name__ == "__main__":
    # Simple test for DOMINANT loss and score tracking
    import torch
    from torch_geometric.data import Data
    # Create a small synthetic graph
    from src.loaders import load_elliptic, make_loader
    t_data = load_elliptic(root="../data/elliptic", force_reload=False,local=True, use_temporal=True, t=1)
 
    t2_data = load_elliptic(root="../data/elliptic", force_reload=False,local=True, use_temporal=True, t=2)

    model = DOMINANT(verbose=3, epoch=3)
   
    model.fit(t_data, eval_data=t2_data)
    print("Train losses:", model.losses_["train"])
    print("Train scores shape:", None if model.scores_["train"] is None else model.scores_["train"].shape)


    print("Eval losses:", model.losses_["eval"])

    print("Eval scores shape:", None if model.scores_["eval"] is None else model.scores_["eval"].shape)