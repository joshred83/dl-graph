{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from pygod.nn import DOMINANTBase  # need DOMINANTBase because DOMINANT doesn't like mini-batching\n",
    "from tqdm.notebook import tqdm  #  \n",
    "\n",
    "# Load data\n",
    "dataset = EllipticBitcoinDataset(root='data/elliptic', force_reload=True)\n",
    "\n",
    "data = dataset[0] # Need to index in because  team-pyg couldnt figure out a less dumb way to load the dataset\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loader = NeighborLoader(data, batch_size=2048, shuffle=True, num_neighbors=[10, 10], input_nodes=data.train_mask) # I think this ithe right way. Should investigate the math. \n",
    "model = DOMINANTBase(in_dim=data.num_node_features).to(device) # I didn't do any tuning. \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # I didn't do any tuning. \n",
    "num_epochs = 20\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, batch in tqdm(enumerate(loader), desc=f\"Epoch {epoch+1}\", leave=True):\n",
    "        batch = batch.to(device)\n",
    "        x = batch.x\n",
    "        edge_index = batch.edge_index\n",
    "        batch_size = getattr(batch, 'batch_size', x.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, s_hat = model(x, edge_index)\n",
    "        s = to_dense_adj(edge_index)[0].to(device)\n",
    "\n",
    "        loss_matrix = model.loss_func(x[:batch_size], x_hat[:batch_size], \n",
    "                                      s[:batch_size, :], s_hat[:batch_size]) # Need to learn the math. \n",
    "        \n",
    "        loss = torch.mean(loss_matrix) # batchwise mean for calculating the gradient\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # but it gets summed for record keeping\n",
    "        \n",
    "\n",
    "    avg_loss = total_loss / i # and then divided so that it's reflective of the full graph\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(loss_history)+1), loss_history, label='Avg Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-graph2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
