{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5329bdb5d89643e3a0391dbd4989a329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Current alpha: 0.6000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689b8b4483014b01acfa0009f54ab163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 9.274e+02, Avg Attribute Loss: 1.749e+01, Avg Structure Loss: 2.292e+03\n",
      "Epoch 2/10\n",
      "Current alpha: 0.5900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ee460c1ad44cfdb6e5d815a40bc9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 1.719e+02, Avg Attribute Loss: 1.519e+01, Avg Structure Loss: 3.975e+02\n",
      "Epoch 3/10\n",
      "Current alpha: 0.5800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c634cc6e32d44a68bf4b7fc952b06d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 7.406e+01, Avg Attribute Loss: 1.479e+01, Avg Structure Loss: 1.559e+02\n",
      "Epoch 4/10\n",
      "Current alpha: 0.5700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c324173e14c4c2ab946198987a6f21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 4.100e+01, Avg Attribute Loss: 1.462e+01, Avg Structure Loss: 7.596e+01\n",
      "Epoch 5/10\n",
      "Current alpha: 0.5600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e319859013954c7da57d36dc1f33c79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 2.657e+01, Avg Attribute Loss: 1.458e+01, Avg Structure Loss: 4.183e+01\n",
      "Epoch 6/10\n",
      "Current alpha: 0.5500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce5c526607743b383413c7c1123015b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 1.841e+01, Avg Attribute Loss: 1.452e+01, Avg Structure Loss: 2.316e+01\n",
      "Epoch 7/10\n",
      "Current alpha: 0.5400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13b6fb70ded4871befe13dc0a1db264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 1.236e+01, Avg Attribute Loss: 1.444e+01, Avg Structure Loss: 9.913e+00\n",
      "Epoch 8/10\n",
      "Current alpha: 0.5300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b9c20ebb2b4aae9b5345c5690d20da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 9.830e+00, Avg Attribute Loss: 1.437e+01, Avg Structure Loss: 4.711e+00\n",
      "Epoch 9/10\n",
      "Current alpha: 0.5200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a557991b00ce4a13b11c04e48ed60274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from dominant import DOMINANTAugmented  # need DOMINANTBase because DOMINANT doesn't like mini-batching\n",
    "from tqdm.notebook import tqdm  #  \n",
    "import datetime\n",
    "\n",
    "# Load data\n",
    "dataset = EllipticBitcoinDataset(root='data/elliptic', force_reload=True)\n",
    "\n",
    "data = dataset[0] # Need to index in because  team-pyg couldnt figure out a less dumb way to load the dataset\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# yo dude, here are hyperparameters for you to play with.\n",
    "dropout = 0.1\n",
    "use_interpolation = True\n",
    "use_perturbation = True\n",
    "interpolation_rate = 0.1\n",
    "feature_noise = 0.05\n",
    "structure_noise = 0.05\n",
    "use_adaptive_alpha = True\n",
    "start_alpha=0.6\n",
    "end_alpha=0.5\n",
    "\n",
    "loader = NeighborLoader(data, batch_size=2048, # we can experiment with different batch sizes too :) \n",
    "                        shuffle=True, num_neighbors=[10, 10], input_nodes=data.train_mask) # I think this ithe right way. Should investigate the math. \n",
    "model = DOMINANTAugmented(dropout=dropout,\n",
    "                        use_interpolation=use_interpolation,\n",
    "                        use_perturbation=use_perturbation,\n",
    "                        interpolation_rate=interpolation_rate,\n",
    "                        feature_noise=feature_noise,\n",
    "                        structure_noise=structure_noise,\n",
    "                        use_adaptive_alpha=use_adaptive_alpha,\n",
    "                        alpha=start_alpha,\n",
    "                        end_alpha=end_alpha,in_dim=data.num_node_features).to(device) # HERE'S SOME TUNING OPTIONS FOR YA <3 - Eric\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Let's also experiment with different learning rates.\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "attr_loss_history = []\n",
    "struct_loss_history = []\n",
    "alpha_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_attr_loss = 0\n",
    "    total_struct_loss = 0\n",
    "\n",
    "\n",
    "    if model.use_adaptive_alpha:\n",
    "        current_alpha = model.update_alpha(epoch, num_epochs)\n",
    "        print(f\"Current alpha: {current_alpha:.4f}\")\n",
    "    else:\n",
    "        current_alpha = model.current_alpha\n",
    "\n",
    "    for i, batch in tqdm(enumerate(loader), desc=f\"Epoch {epoch+1}\", leave=True):\n",
    "        batch = batch.to(device)\n",
    "        x = batch.x\n",
    "        edge_index = batch.edge_index\n",
    "        batch_size = getattr(batch, 'batch_size', x.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, s_hat = model(x, edge_index)\n",
    "        s = to_dense_adj(edge_index)[0].to(device)\n",
    "\n",
    "        loss_matrix = model.compute_loss(\n",
    "            x[:batch_size], \n",
    "            x_hat[:batch_size],\n",
    "            s[:batch_size, :], \n",
    "            s_hat[:batch_size]\n",
    "        ) \n",
    "        \n",
    "        loss = torch.mean(loss_matrix) # batchwise mean for calculating the gradient\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # gradient clipping - eric\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # but it gets summed for record keeping\n",
    "\n",
    "        with torch.no_grad(): # since we're just updating the loss scorekeeping\n",
    "            # Attribute loss \n",
    "            attr_matrix = model.loss_func(\n",
    "                x[:batch_size], \n",
    "                x_hat[:batch_size],\n",
    "                s[:batch_size, :], \n",
    "                s_hat[:batch_size],\n",
    "                weight=1.0, #  from the docstring: `Balancing weight... between 0 and 1 inclusive between node feature and graph structure.`\n",
    "                # logically, it's set to 1.0 here because we want to calculate the attribute loss\n",
    "                # below, we will set it to 0 to calculate the structure loss\n",
    "                pos_weight_a=model.pos_weight_a,\n",
    "                pos_weight_s=model.pos_weight_s,\n",
    "                bce_s=model.bce_s\n",
    "            )\n",
    "            attr_loss = torch.mean(attr_matrix).item()\n",
    "            total_attr_loss += attr_loss\n",
    "            \n",
    "            # Structure loss \n",
    "            struct_matrix = model.loss_func(\n",
    "                x[:batch_size], \n",
    "                x_hat[:batch_size],\n",
    "                s[:batch_size, :], \n",
    "                s_hat[:batch_size],\n",
    "                weight=0.0,\n",
    "                pos_weight_a=model.pos_weight_a,\n",
    "                pos_weight_s=model.pos_weight_s,\n",
    "                bce_s=model.bce_s\n",
    "            )\n",
    "            struct_loss = torch.mean(struct_matrix).item()\n",
    "            total_struct_loss += struct_loss\n",
    "        \n",
    "\n",
    "    batch_count = i + 1  # Total number of batches\n",
    "    avg_loss = total_loss / batch_count\n",
    "    avg_attr_loss = total_attr_loss / batch_count\n",
    "    avg_struct_loss = total_struct_loss / batch_count\n",
    "    print(f\"Avg Loss: {avg_loss:.3e}, \"\n",
    "          f\"Avg Attribute Loss: {avg_attr_loss:.3e}, \"\n",
    "          f\"Avg Structure Loss: {avg_struct_loss:.3e}\")\n",
    "    \n",
    "    # Record losses\n",
    "    loss_history.append(avg_loss)\n",
    "    attr_loss_history.append(avg_attr_loss)\n",
    "    struct_loss_history.append(avg_struct_loss)\n",
    "    alpha_history.append(current_alpha)\n",
    "\n",
    "    avg_loss = total_loss / i # and then divided so that it's reflective of the full graph\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(loss_history)+1), loss_history, 'o-', label='Total Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log') # log scale for better visibility\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "now = datetime.datetime.now()\n",
    "formatted_now = now.strftime(\"%Y-%m-%d_%H_%M\")\n",
    "plt.savefig(f\"loss_plot_{formatted_now}.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-graph2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
